embedding_size: 64
feat_embed_dim: 64
weight_size: [64, 64]

learning_rate_scheduler: [0.96, 50]
lambda_coeff: 0.9
reg_weight: [1e-02]

cf_model: lightgcn
mess_dropout: [0.1, 0.1]
n_layers: [1]
knn_k: 10

# find the best encoder for each modality
# we set this encoder on the text feature file, while we keep visual encoder empty
# this avoids code fsils

# text_feature_file: ["text/minilm.npy", "text/mpnet.npy", "text/clip_text.npy"]
# text_feature_file: ["image/vit.npy", "image/vgg.npy", "image/clip_image.npy"]
# text_feature_file: ["video/mvit.npy", "video/r2p1d.npy", "video/slowfast.npy"]
#Â text_feature_file: ["audio/ast.npy", "audio/vggish.npy", "audio/whisper.npy"]

# vision_feature_file: ''

# now that we have the best single encoder per modality, 
# we run the experiments with each possible pair of these 4 modalities

# (a) text/clip_text.npy
# (b) image/vgg.npy
# (c) audio/ast.npy
# (d) video/mvit.npy

text_feature_file: ["audio/ast.npy"]
vision_feature_file: ["video/mvit.npy"]

hyper_parameters: ["reg_weight", "n_layers", "text_feature_file", "vision_feature_file"]

