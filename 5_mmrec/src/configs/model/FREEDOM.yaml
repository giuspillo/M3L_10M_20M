embedding_size: 64
feat_embed_dim: 64
weight_size: [64, 64]

lambda_coeff: 0.9
reg_weight: [1e-02]

n_mm_layers: [1,2]
n_ui_layers: [1]
knn_k: 10

mm_image_weight: 0.1
dropout: 0.8

# find the best encoder for each modality
# we set this encoder on the text feature file, while we keep visual encoder empty
# this avoids code fsils

# text_feature_file: ["text/minilm.npy", "text/mpnet.npy", "text/clip_text.npy"]
# text_feature_file: ["image/vit.npy", "image/vgg.npy", "image/clip_image.npy"]
# text_feature_file: ["video/mvit.npy", "video/r2p1d.npy", "video/slowfast.npy"]
#Â text_feature_file: ["audio/ast.npy", "audio/vggish.npy", "audio/whisper.npy"]
# text_feature_file: ["audio/whisper.npy"]

# vision_feature_file: ''

# now that we have the best single encoder per modality, 
# we run the experiments with each possible pair of these 4 modalities

# (a) text/mpnet.npy
# (b) image/vgg.npy
# (c) audio/vggish.npy
# (d) video/mvit.npy

text_feature_file: ["audio/vggish.npy"]
vision_feature_file: ["video/mvit.npy"]

hyper_parameters: ["reg_weight", "n_ui_layers", "n_mm_layers", "text_feature_file", "vision_feature_file"]
